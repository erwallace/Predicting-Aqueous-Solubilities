{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cheminformatics Project: Predicting Aqueous Solubility using Linear Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "ec59bb0e",
   "metadata": {},
   "source": [
    "Cheminformatics is an exciting field that combines chemistry with informatics by collecting, storing, analysing and manipulating chemical data. Cheminformatics originally emerged to aid and inform drug discovery, however it has now grown to involve important areas of biology, chemistry and biochemistry, with many similarities to the field of bioinformatics.\n",
    "\n",
    "In this project we will first try to replicate, and then improve upon, research by John S. Delaney [1], who used linear regression to predict the aqueous solubility (which he calls ESOL values) of a range of molecules. Delaney demonstrates that you can predict the solubility to a reasonable accuracy using 4 easily observable features: molecular weight, number of rotatable bonds, aromatic proportion and the octane-water partition coefficient.\n",
    "\n",
    "Aqueous solubility is one of the key physical properties of interest to a medicinal chemist as it affects the uptake/distribution of biologically active compounds within the body.\n",
    "\n",
    "Finally, I would like to thank Prof. Jan H. Jensen [2] for the idea for this notebook and for his excellent tutorials on the use of machine learning within chemistry."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Libraries**\n",
    "\n",
    "**1.0 Reproducing Delaney's Results**\n",
    "1.1 Data Creation\n",
    "1.2 Data Analysis\n",
    "1.3 Model\n",
    "1.4 Evaluation\n",
    "\n",
    "**2.0 Improving upon Delaney's Results**\n",
    "2.1 Additional Data Creation\n",
    "2.2 Additional Data Analysis\n",
    "2.3 Model\n",
    "2.3.1 Additional Features\n",
    "2.3.2 Tuning Hyper Parameters\n",
    "2.4 Evaluation\n",
    "2.5 Conclusion\n",
    "\n",
    "**References**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for kaggle\n",
    "# import os\n",
    "# !conda install -c rdkit rdkit -y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Libraries**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed9d87",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from rdkit.Chem import PandasTools, Descriptors, rdMolDescriptors\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_validate, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_15484/3709755123.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandom\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseed\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "97d2519e",
   "metadata": {},
   "source": [
    "# 1.0 Reproducing Delaney's Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d04027",
   "metadata": {},
   "source": [
    "# 1.1 Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Importing Delaney's Dataset**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data used in Delany's regression can be found in his paper's [supporting information](https://pubs.acs.org/doi/suppl/10.1021/ci034243x/suppl_file/ci034243xsi20040112_053635.txt) as a csv. However, it is also possible to generate all of these features using the python cheminformatics library, RDKit. Molecules are most commonly input into RDKit SMILE (Simplified Molecular-Input Line-Entry System) strings, which are used to create RDKit objects."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73656af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m wget https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('delaney-processed.csv',\n",
    "                 usecols=['Compound ID',\n",
    "                          'smiles',\n",
    "                          'ESOL predicted log solubility in mols per litre',\n",
    "                          'measured log solubility in mols per litre'])\n",
    "\n",
    "df = df.rename(columns={'Compound ID':'Name',\n",
    "                        'ESOL predicted log solubility in mols per litre':'ESOL Predicted log(solubility/M)',\n",
    "                        'measured log solubility in mols per litre':'Measured log(solubility/M)'})\n",
    "# Molar (M) is an equivalent unit to mols per litre (or dm^3).\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e13c845",
   "metadata": {},
   "source": [
    "**Deriving Inputs from RDKit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efd5b6",
   "metadata": {},
   "source": [
    "Using the SMILE strings we will create an RDKit object for each molecule. This then allows us to generate all of the features we'll need with just a few lines of code. This is the essence of Delaney's paper, providing a way to predict solubility values using only easily generated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Type: rdkit.Chem.rdchem.Mol, displayed as an image in df\n",
    "PandasTools.AddMoleculeColumnToFrame(df, 'smiles', 'Molecule')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1afc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aromatic_proportion(mol):\n",
    "    \n",
    "    aromatic_atoms = [mol.GetAtomWithIdx(i).GetIsAromatic() for i in range(mol.GetNumAtoms())] #returns bool for each atom\n",
    "    aromatic_count = sum(aromatic_atoms)\n",
    "    heavy = Descriptors.HeavyAtomCount(mol)\n",
    "    \n",
    "    return aromatic_count / heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# molecular weight\n",
    "mw_lst = [ Descriptors.MolWt(element) for element in df.Molecule ]\n",
    "df['MW'] = np.array(mw_lst)\n",
    "\n",
    "# number of rotatable bonds\n",
    "rb_lst = [ Descriptors.NumRotatableBonds(element) for element in df.Molecule ]\n",
    "df['RB'] = np.array(rb_lst)\n",
    "\n",
    "# aromatic proportion \n",
    "ap_lst = [ aromatic_proportion(element) for element in df.Molecule ]\n",
    "df['AP'] = np.array(ap_lst)\n",
    "\n",
    "# octane-water partition coefficient\n",
    "cLogP_lst = [ Descriptors.MolLogP(element) for element in df.Molecule ]\n",
    "df['cLogP'] = np.array(cLogP_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have all of the training data needed, we can begin by replicating Delaney's results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.2 Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe().T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we will plot histograms and scatter plots for each feature to gain an overview of the characteristics of the molecules in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histograms of each variable and the measured log solubility\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10), sharey=True)\n",
    "\n",
    "ax1 = ax[0,0]\n",
    "ax1.hist(df['MW'], bins=list(range(0,850,50)), rwidth=0.9, color='b')\n",
    "ax1.set_xlabel('Molecular Weight')\n",
    "ax1.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax2 = ax[0,1]\n",
    "ax2.hist(df['RB'], bins=list(range(23)), align='left', rwidth=0.9, color='r')\n",
    "ax2.set_xlabel('Rotatable Bonds')\n",
    "ax2.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax3 = ax[1,0]\n",
    "ax3.hist(df['AP'], bins=20, rwidth=0.9, color='g')\n",
    "ax3.set_xlabel('Aromatic Proportion')\n",
    "ax3.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax4 = ax[1,1]\n",
    "ax4.hist(df['cLogP'], bins=25, align='left', rwidth=0.9, color='m')\n",
    "ax4.set_xlabel('Octane-Water Partition Coefficient')\n",
    "ax4.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# scatter plot for each variable and the log solubility\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "\n",
    "ax1 = ax[0,0]\n",
    "ax1.scatter(df['MW'], df['Measured log(solubility/M)'], c='b', alpha=0.2)\n",
    "ax1.set_xlabel('Molecular Weight')\n",
    "ax1.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax2 = ax[0,1]\n",
    "sns.swarmplot(data=df[df.RB<11], x='RB', y='Measured log(solubility/M)', ax=ax[0,1], c='r', s=2.4, alpha=0.8)\n",
    "# values above 10 not shown as there are less than 5 molecules for each of these values\n",
    "ax2.set_xlabel('No. Rotatable Bonds')\n",
    "ax2.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax3 = ax[1,0]\n",
    "ax3.scatter(df['AP'], df['Measured log(solubility/M)'], c='g', alpha=0.2)\n",
    "ax3.set_xlabel('Aromatic Proportion')\n",
    "ax3.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax4 = ax[1,1]\n",
    "ax4.scatter(df['cLogP'], df['Measured log(solubility/M)'], c='m', alpha=0.2)\n",
    "ax4.set_xlabel('Octane-Water Partition Coefficient')\n",
    "ax4.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For discrete data, as in number of rotatable bonds, a swarm plot is used instead of a scatter plot."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Even before training our algorithm we can see that the octane-water partition coefficient is likely to be an important factor in the regression as it has a clear linear correlation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "2a86bab5",
   "metadata": {},
   "source": [
    "# 1.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will first train a linear regression using the four features from Delaney's paper."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df[['MW', 'RB', 'AP', 'cLogP']]\n",
    "\n",
    "Y = df['Measured log(solubility/M)']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab071f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearRegression()\n",
    "k = 10\n",
    "repeats = 3\n",
    "cv = RepeatedKFold(n_splits=k, n_repeats=repeats, random_state=1)\n",
    "scores = cross_validate(model, X_train, Y_train, cv=cv,\n",
    "                        scoring=('neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517959cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "mse = -np.mean(scores[\"test_neg_mean_squared_error\"])\n",
    "mae = -np.mean(scores[\"test_neg_mean_absolute_error\"])\n",
    "r2 = np.mean(scores[\"test_r2\"])\n",
    "# standard errors of metrics\n",
    "mse_se = np.std(scores[\"test_neg_mean_squared_error\"])/np.sqrt(k*repeats)\n",
    "mae_se = np.std(scores[\"test_neg_mean_absolute_error\"])/np.sqrt(k*repeats)\n",
    "r2_se = np.std(scores[\"test_r2\"])/np.sqrt(k*repeats)\n",
    "\n",
    "print(f'Mean squared error (MSE): {mse:.2f} ± {ceil(mse_se*100)/100:.2f}')\n",
    "print(f'Mean absolute error (MAE): {mae:.2f} ± {ceil(mae_se*100)/100:.2f}')\n",
    "print(f'Coefficient of determination (R^2): {r2:.2f} ± {ceil(r2_se*100)/100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Delaney_sklearn = { 'model':'sklearn_basic',\n",
    "                    'MSE':mse,\n",
    "                    'MAE':mae,\n",
    "                    'R^2':r2,\n",
    "                    'MSE_se': mse_se,\n",
    "                    'MAE_se': mae_se,\n",
    "                    'R^2_se': r2_se,\n",
    "                    'intercept':model.intercept_,\n",
    "                    'MW':model.coef_[0],\n",
    "                    'RB':model.coef_[1],\n",
    "                    'AP':model.coef_[2],\n",
    "                    'cLogP':model.coef_[3] }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "980b1150",
   "metadata": {},
   "source": [
    "# 1.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate delaney's stats\n",
    "delaney_error = df['Measured log(solubility/M)']-df['ESOL Predicted log(solubility/M)']\n",
    "d_mse = np.mean(delaney_error**2)\n",
    "d_mae = np.mean(abs(delaney_error))\n",
    "\n",
    "diff = df['Measured log(solubility/M)']-df['ESOL Predicted log(solubility/M)'].mean()\n",
    "d_r2 = 1 - (np.sum(delaney_error**2)/np.sum(diff**2))\n",
    "\n",
    "Delaney_literature = {'model':'literature',\n",
    "                      'MSE':d_mse,\n",
    "                      'MAE':d_mae,\n",
    "                      'R^2':d_r2,\n",
    "                      'MSE_se': np.nan,\n",
    "                      'MAE_se': np.nan,\n",
    "                      'R^2_se': np.nan,\n",
    "                      'intercept':0.16,\n",
    "                      'MW':-0.0062,\n",
    "                      'RB':0.066,\n",
    "                      'AP':-0.74,\n",
    "                      'cLogP':-0.63 }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.DataFrame([Delaney_literature, Delaney_sklearn]).set_index('model')\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " Before plotting the weights of each feature we shall normalise them by multiply them by their mean. Otherwise features such as molecular weight, that has a large mean of 204, will appear to have contributed less to the model than other feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "norm_results = results\n",
    "for feature in df.columns:\n",
    "    if feature in norm_results.columns:\n",
    "        norm_results[feature] = norm_results[feature] * df[feature].mean()\n",
    "norm_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot of a summary of the three models\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "width = 0.4\n",
    "\n",
    "# summary of statistics\n",
    "ax1 = ax[0]\n",
    "\n",
    "X = results.columns[:3]\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "ax1.bar(X_axis-width/2, results.loc['literature'][:3], width=width, label = 'literature')\n",
    "ax1.bar(X_axis+width/2, results.loc['sklearn_basic'][:3], width=width, label = 'sklearn', yerr=results.loc['sklearn_basic'][3:6], capsize=5)\n",
    "\n",
    "ax1.set_xticks(X_axis, X)\n",
    "ax1.set_title(\"Summary of Statistics\")\n",
    "ax1.legend()\n",
    "\n",
    "# summary of coefficients\n",
    "ax2 = ax[1]\n",
    "\n",
    "X = results.columns[6:]\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "ax2.bar(X_axis-width/2, norm_results.loc['literature'][6:], width=width, label = 'literature')\n",
    "ax2.bar(X_axis+width/2, norm_results.loc['sklearn_basic'][6:], width=width, label = 'sklearn')\n",
    "\n",
    "ax2.set_xticks(X_axis, X)\n",
    "ax2.set_title(\"Summary of Coefficients\")\n",
    "ax2.set_ylabel('Normalised Feature Coefficients')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The major points demonstrated by the summary of statistics are:\n",
    "- Delaney's model is significantly more accurate for across all indicators\n",
    "- Errors for MAE and R^2 are smaller by a factor of 3 than the errors for MSE.\n",
    "\n",
    "And the main points from the summary of coefficients are:\n",
    "- As suggested by the scatter plots, the octane-water partition coefficient and molecular weight are given the most weight in my models.\n",
    "- The coefficient for the number of rotatable bonds, although still small, is an order of magnitude higher is Delaney's AND the opposite sign.\n",
    "\n",
    "These observed differences between mine and Delaney's models can be explained in part by the difference in training set size. Delaney had 2874 molecules, whereas the supplementary information only gives us access to 1128."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "d48c3a33",
   "metadata": {},
   "source": [
    "# 2.0 Improving upon Delaney's Results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We shall now try to optimise our linear regression to see if we can improve on it's accuaracy and perhaps even beat Delaney's model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.1 Additional Data Creation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One simple way to improve a linear regression is to add more features (up to a certain point). We will try adding 3 more descriptors using RDKit: number of rings, number of H bond donors and the topological polar surface area."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1603ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rings\n",
    "rings_lst = [ rdMolDescriptors.CalcNumRings(element) for element in df.Molecule ]\n",
    "df['RINGS'] = np.array(rings_lst)\n",
    "\n",
    "# number of H bond donors\n",
    "hbd_lst = [ rdMolDescriptors.CalcNumHBD(element) for element in df.Molecule ]\n",
    "df['HBD'] = np.array(hbd_lst)\n",
    "\n",
    "# topological polar surface area\n",
    "psa_lst = [ Descriptors.TPSA(element) for element in df.Molecule ]\n",
    "df['PSA'] = np.array(psa_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Additional Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once again we will plot these new features to give an indication of their distributions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe().T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# histograms of each variable and the log solubility\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "ax1 = ax[0]\n",
    "ax1.hist(df['RINGS'], bins=8, align='left', rwidth=0.9, color='c')\n",
    "ax1.set_xlabel('No. Rings')\n",
    "ax1.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax2 = ax[1]\n",
    "ax2.hist(df['HBD'], bins=11, align='left', rwidth=0.9, color='y')\n",
    "ax2.set_xlabel('No. H Bond Donors')\n",
    "ax2.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax3 = ax[2]\n",
    "ax3.hist(df['PSA'], bins=15, rwidth=0.9, color='k')\n",
    "ax3.set_xlabel('Topological Polar Surface Area')\n",
    "ax3.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot for each variable and the log solubility\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\n",
    "\n",
    "ax1 = ax[0]\n",
    "sns.swarmplot(data=df, x='RINGS', y='Measured log(solubility/M)', ax=ax[0], c='c', s=1.8, alpha=0.8)\n",
    "ax1.set_xlabel('No. Rings')\n",
    "ax1.set_ylabel('Measured log(solubility/M)')\n",
    "\n",
    "ax2 = ax[1]\n",
    "sns.swarmplot(data=df, x='HBD', y='Measured log(solubility/M)', ax=ax[1], c='y', s=1.6, alpha=0.8)\n",
    "ax2.set_xlabel('No. H Bond Donors')\n",
    "\n",
    "ax3 = ax[2]\n",
    "ax3.scatter(df['PSA'], df['Measured log(solubility/M)'], c='k', alpha=0.2, s=30)\n",
    "ax3.set_xlabel('Topological Polar Surface Area')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# correlation table\n",
    "corr = df[['Measured log(solubility/M)', 'MW', 'RB', 'AP', 'cLogP', 'RINGS', 'HBD', 'PSA']].corr()\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.heatmap(corr, cmap='mako', annot=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Correlation plots are useful for checking for collinearity between features. This plot demonstrates why cLogP is given the most weight within the linear regression, as it has an absolute correlation of 0.83."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "8bb71239",
   "metadata": {},
   "source": [
    "# 2.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3.1 Additional Features**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now train a new model with these additional features to see if it improves the accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['MW', 'RB', 'AP', 'cLogP', 'RINGS', 'HBD', 'PSA']]\n",
    "\n",
    "Y = df['Measured log(solubility/M)']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741c2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearRegression()\n",
    "cv = RepeatedKFold(n_splits=k, n_repeats=repeats, random_state=1)\n",
    "scores = cross_validate(model, X_train, Y_train, cv=cv,\n",
    "                        scoring=('neg_mean_squared_error', 'neg_mean_absolute_error', 'r2'))\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# metrics\n",
    "mse = -np.mean(scores[\"test_neg_mean_squared_error\"])\n",
    "mae = -np.mean(scores[\"test_neg_mean_absolute_error\"])\n",
    "r2 = np.mean(scores[\"test_r2\"])\n",
    "# standard errors of metrics\n",
    "mse_se = np.std(scores[\"test_neg_mean_squared_error\"]) / np.sqrt(k*repeats)\n",
    "mae_se = np.std(scores[\"test_neg_mean_absolute_error\"]) / np.sqrt(k*repeats)\n",
    "r2_se = np.std(scores[\"test_r2\"]) / np.sqrt(k*repeats)\n",
    "\n",
    "print(f'Mean squared error (MSE): {mse:.2f} ± {ceil(mse_se * 100) / 100:.2f}')\n",
    "print(f'Mean absolute error (MAE): {mae:.2f} ± {ceil(mae_se * 100) / 100:.2f}')\n",
    "print(f'Coefficient of determination (R^2): {r2:.2f} ± {ceil(r2_se * 100) / 100:.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "improved_sklearn = {'model': 'sklearn_improved',\n",
    "                    'MSE': mse,\n",
    "                    'MAE': mae,\n",
    "                    'R^2': r2,\n",
    "                    'MSE_se': mse_se,\n",
    "                    'MAE_se': mae_se,\n",
    "                    'R^2_se': r2_se,\n",
    "                    'intercept': model.intercept_,\n",
    "                    'MW': model.coef_[0],\n",
    "                    'RB': model.coef_[1],\n",
    "                    'AP': model.coef_[2],\n",
    "                    'cLogP': model.coef_[3],\n",
    "                    'RINGS':model.coef_[4],\n",
    "                    'HBD':model.coef_[5],\n",
    "                    'PSA':model.coef_[6]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2.3.2 Tuning Hyper Parameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although linear regression has very few hyper-parameters, the choice of features is something we can tune. This may increase the accuracy of our plot, particularly as certain features (such as PSA and HBD) are quite highly correlated. We will do this using recursive feature elimination (RFE). Choosing fewer features can allow machine learning algorithms to run more efficiently (less space or time complexity) and be more effective.\n",
    "\n",
    "First RFECV (recursive feature elimination cross validation) can be used to select the best model and best number of features. This is done by recursively reducing the number of features chosen until the accuracy decreases. Then we will take the best model and see how it ranks features and what the optimum number of features is.\n",
    "\n",
    "MAE will be used to compare models as it has a substantially lower standard error than MSE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results, names = [], []\n",
    "\n",
    "models = {'lr': LinearRegression(),\n",
    "          'tree': DecisionTreeRegressor(),\n",
    "          'rf': RandomForestRegressor(),\n",
    "          'gbm': GradientBoostingRegressor()}\n",
    "\n",
    "results, names = [], []\n",
    "for name, feature_selection in models.items():\n",
    "\n",
    "    rfecv = RFECV(estimator=feature_selection)\n",
    "    rfecv.fit(X_train, Y_train)\n",
    "    model = LinearRegression()\n",
    "    pipe = Pipeline(steps=[('s',rfecv),('m',model)])\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=repeats, random_state=1)\n",
    "    scores = -cross_val_score(pipe, X_train, Y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    results.append(scores)\n",
    "    names.append(name)\n",
    "\n",
    "    print(f'> model: {name}, {sum(rfecv.support_)} features, mse: {np.mean(scores):.3f} ± {np.std(scores)/np.sqrt(k*repeats):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear regression with all 7 features performs the best out of the models tested. However, it's worth noting that all models tested are within the standard error of the linear regression and so the choice of model is somewhat arbitrary.\n",
    "\n",
    "Anyway, let's see how the number of features affects the MSE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "models = {}\n",
    "results, names = [], []\n",
    "for i in range(2, X_train.shape[1]+1):\n",
    "\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=i)\n",
    "    model = LinearRegression()\n",
    "    pipe = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "\n",
    "    cv = RepeatedKFold(n_splits=k, n_repeats=repeats, random_state=1)\n",
    "    scores = -cross_val_score(pipe, X_train, Y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "    results.append(scores)\n",
    "    names.append(i)\n",
    "\n",
    "    print(f'>no. features: {i}, mse: {np.mean(scores):.3f} ± {np.std(scores)/np.sqrt(k*repeats):.3f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot model performance for comparison\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.xlabel('Number of Features used in Linear Regression')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most accurate linear regression is the one with all 7 features (i.e. our sklearn_improved model). However, this is still a useful check to have done since several features had relatively high correlations (e.g. 0.73 for PSA and HBD)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.4 Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = pd.DataFrame([Delaney_literature, Delaney_sklearn, improved_sklearn]).set_index('model')\n",
    "results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "norm_results = results\n",
    "for feature in df.columns:\n",
    "    if feature in norm_results.columns:\n",
    "        norm_results[feature] = norm_results[feature] * df[feature].mean()\n",
    "norm_results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot of a summary of the three models\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "width = 0.25\n",
    "\n",
    "# summary of statistics\n",
    "ax1 = ax[0]\n",
    "\n",
    "X = results.columns[:3]\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "ax1.bar(X_axis-width, results.loc['literature'][:3], width=width, label = 'literature')\n",
    "ax1.bar(X_axis, results.loc['sklearn_basic'][:3], width=width, label = 'basic', yerr=results.loc['sklearn_basic'][3:6], capsize=5)\n",
    "ax1.bar(X_axis+width, results.loc['sklearn_improved'][:3], width=width, label = 'improved', yerr=results.loc['sklearn_improved'][3:6], capsize=5)\n",
    "\n",
    "ax1.set_xticks(X_axis, X)\n",
    "ax1.set_title(\"Summary of Statistics\")\n",
    "ax1.legend()\n",
    "\n",
    "# summary of common coefficients\n",
    "ax2 = ax[1]\n",
    "\n",
    "X = results.columns[6:11]\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "ax2.bar(X_axis-width, norm_results.loc['literature'][6:11], width=width, label = 'literature')\n",
    "ax2.bar(X_axis, norm_results.loc['sklearn_basic'][6:11], width=width, label = 'basic')\n",
    "ax2.bar(X_axis+width, norm_results.loc['sklearn_improved'][6:11], width=width, label = 'improved')\n",
    "\n",
    "ax2.set_xticks(X_axis, X)\n",
    "ax2.set_title(\"Summary of Common Coefficients\")\n",
    "ax2.legend()\n",
    "\n",
    "# summary of all coefficients for improved sklearn\n",
    "ax3 = ax[2]\n",
    "\n",
    "X = results.columns[6:]\n",
    "X_axis = np.arange(len(X))\n",
    "\n",
    "ax3.bar(X_axis, norm_results.loc['sklearn_improved'][6:], label = 'improved', color='tab:green')\n",
    "\n",
    "ax3.set_xticks(X_axis, X, rotation=45, ha='center')\n",
    "ax3.set_title(\"Summary of All Coefficients for Improved\")\n",
    "ax3.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see adding more features immediately reduces both the MSE (by 11%) and the MAE (by 4%). This also further increases the models reliance on the octane-water partition coefficient and reduces the reliance on aromatic proportion and molecular weight, moving further away from Delaney's coefficients.\n",
    "\n",
    "This can be partially explained by the inclusion of the number of rings as a feature. Most rings will be aromatic and so rings and AP will be correlated to some degree. In fact, if you add together the coefficients for rings and aromatic proportion you get a value very close to that of the aromatic proportion coefficient for previous models.\n",
    "\n",
    "We will now plot the absolute weights of each feature in order of RFE ranking to determine if there is a correlation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# report which features were selected by RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "feature_selection = pd.DataFrame(index=X_train.columns)\n",
    "\n",
    "for num in range(X_train.shape[1], 0, -1):\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=num)\n",
    "    rfe.fit(X_train, Y_train)\n",
    "    feature_selection['ranking'] = rfe.ranking_\n",
    "    feature_selection[f'n={num}'] = rfe.support_\n",
    "\n",
    "feature_selection = feature_selection.sort_values(by='ranking')\n",
    "print(feature_selection)\n",
    "\n",
    "plot = []\n",
    "for feature in list(feature_selection.index):\n",
    "    plot.append(abs(norm_results.loc['sklearn_improved', feature]))\n",
    "\n",
    "X = list(feature_selection.index)\n",
    "X_axis = np.arange(len(X))\n",
    "plt.bar(X_axis, plot, label = 'improved', color='tab:green')\n",
    "plt.xticks(X_axis, X)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Normalised Weights from sklearn_improved')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, while cLogP has both the largest absolute weight and RFE ranking, there seems to be very little correlation between the two.\n",
    "\n",
    "For a final comparison we will plot predicted vs measure values for both Delaney's values and for our sklearn improved values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "\n",
    "ax1 = ax[0]\n",
    "ax1.scatter(df['Measured log(solubility/M)'], df['ESOL Predicted log(solubility/M)'], alpha=0.5)\n",
    "ax1.set_xlabel('Measured')\n",
    "ax1.set_ylabel('Predicted')\n",
    "ax1.set_title(\"Delaney's Literature Values\")\n",
    "ax1.set_ylim(-10,2)\n",
    "ax1.set_xlim(-12,2)\n",
    "\n",
    "ax2 = ax[1]\n",
    "ax2.scatter(Y_test, Y_pred_test, alpha=0.5, color='tab:red')\n",
    "ax2.set_xlabel('Measured')\n",
    "ax2.set_ylabel('Predicted')\n",
    "ax2.set_title(\"sklearn Improved Values\")\n",
    "ax2.set_ylim(-10,2)\n",
    "ax2.set_xlim(-12,2)\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.5 Conclusion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "ce2aa30c",
   "metadata": {},
   "source": [
    "Despite the inclusion of additional features, we could not quite beat the accuracy of Delaney's model. As stated before this is most likely due to using a smaller training set. It is worth mentioning that while Delaney's data spans 13 logs of solubility, most experimental data will only span 2 or 3, meaning that even with a MAE of 0.70, Delaney's model is still only an estimate.\n",
    "\n",
    "However, ESOL is a really quick, easy and useful technique for predicting aqueous solubility, particularly when developing new molecules. This is because the most widely used model for predicting aqueous solubility, the General Solubility Equation (GSE) relies on knowing the melting temperature of a molecule. While melting temperature can be measured for existing molecules, predicting it for *de novo* molecules is notoriously tricky and computationally expensive.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. [ESOL: Estimating Aqueous Solubility Directly from Molecular Structure](https://pubs.acs.org/doi/10.1021/ci034243x), J. S. Delany, J. Chem. Inf. Comput. Sci. 2004, **44**, 1000-1005\n",
    "2. [Machine Learning Basics by Jan H. Jensen](https://sites.google.com/view/ml-basics/home?authuser=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
